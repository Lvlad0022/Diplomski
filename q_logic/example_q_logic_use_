import torch.optim as optim
import torch
import numpy as np
from q_logic.q_logic import Agent
from math import cos

from models import DQN, DQNnoisy, DQNnoisy2
from q_logic.loss_functions import huberLoss
from q_logic.q_logic_memory_classes import TDPriorityReplayBuffer, ReplayBuffer
from q_logic.q_logic_schedulers import WarmupPeakDecayScheduler


"""
example of user agent and all the functions it has to have
"""


class userAgent(Agent): #inheriting from q_logic Agent 
    def __init__(self, train = True,n_step_remember=1,  gamma=0.93, priority = False, memory = 0, advanced_logging_path= False, time_logging_path = False, model = None, double_q = False, polyak = True, noisy_net = False):
        
        model =DQNnoisy(is_training=True) if noisy_net else DQN()
         
        optimizer = optim.Adam(model.parameters(),lr=5e-4)
        scheduler = WarmupPeakDecayScheduler(optimizer,max_lr=5e-4, final_lr=5e-6, initial_lr=5e-4, decay_steps= 500_000)

        memory = ReplayBuffer() # memory needs to be from given memory options in q_logic_memory_classes
                                # it is either uniform memory or td error priority
        if priority:
            memory = TDPriorityReplayBuffer()

        possible_actions = [0,1,2,3] # you need to give possible actions to the agent actions can be any python object
                
        super().__init__(model = model, polyak_update=polyak, gamma = gamma, optimizer = optimizer, scheduler=scheduler, advanced_logging_path= advanced_logging_path,possible_actions =possible_actions,
                         criterion= huberLoss(), train = train, n_step_remember=n_step_remember, memory=memory, batch_size=64, double_q = double_q, noisy_net=noisy_net)  # pozove konstruktor od Agent
    
    """
    every agent has to have theese functions:
    """

    def give_reward(self, data_novi, data, akcija):
        # costumize reward function, needs to return a number for reward and boolian done if episude ended this move or not
        data_novi, snake_state,reward, jabuka,done = data_novi
        if done:
            reward = -1
        elif jabuka == 50:
            done = True
            reward = 1
        elif reward < 1:
            reward = -0.05
        else:
            reward = 1
        return reward, done

    def get_state(self, data):
        """
        returns a dictionary of torch tensors used for pytorch training
        keys NEED to have same names as parameters in pytorch model forward function that will be used
        """
        data, snake_state, reward, jabuka, done = data
        return {"x": torch.tensor(np.array(data))}
    
    def memory_to_model(self, memory_state):
        """
        returns state in suitable data structure that takes up small amount of space 
        if you want a lot of emmory in the buffer and you are limited on RAM
        """
        return memory_state

    def get_memory_state(self, data):
        """
        takes  memory to model output and
        outputs same output as get state
        in this example there was no need for better memory data usage 
        """
        return self.get_state(data)
        



class snakeAgent2(Agent):
    def __init__(self, train = True,n_step_remember=1,  gamma=0.93, priority = False, memory = 0, advanced_logging_path= False, time_logging_path = False, model = None, double_q = False, polyak = True, noisy_net = False):
        self.reward_policy = True
        self.survive_reward = 0
        model =DQNnoisy2(is_training=True, map_channels=4) if noisy_net else DQN(map_channels=4)
         
        optimizer = optim.Adam(model.parameters(),lr=5e-4)
        scheduler = WarmupPeakDecayScheduler(optimizer,max_lr=5e-4, final_lr=5e-6, initial_lr=5e-4, decay_steps= 50_000)

        memory = ReplayBuffer()
        if priority:
            memory = TDPriorityReplayBuffer()

        possible_actions = [0,1,2,3]
                
        super().__init__(model = model, polyak_update=polyak, gamma = gamma, optimizer = optimizer,scheduler=scheduler, advanced_logging_path= advanced_logging_path,possible_actions =possible_actions,
                         criterion= huberLoss(), train = train, n_step_remember=n_step_remember, memory=memory, batch_size=64, double_q = double_q, noisy_net=noisy_net)  # pozove konstruktor od Agent
        

    def give_reward(self, data_novi, data, akcija):
        data_novi,snake_state, reward, jabuka,done = data_novi
        if self.reward_policy == True:
            if done:
                reward = -1
            elif jabuka == 50:
                done = True
                reward = 1
            elif reward < 1:
                reward = self.survive_reward
            else:
                reward = 1
        return reward, done

    def get_state(self, data):
        data,snake_state, reward, jabuka, done = data
        tenzor = np.zeros((4,data.shape[1],data.shape[2]))
        tenzor[:3,:,:] = data
        n = len(snake_state)-1
        for i, (x,y) in enumerate(snake_state[1:]):
            tenzor[3,x,y] = (n-i)/100
        return {"x": torch.tensor(tenzor, dtype=torch.float32)}
    
    def memory_to_model(self, memory_state):
        return memory_state

    def get_memory_state(self, data):
        return self.get_state(data)
        